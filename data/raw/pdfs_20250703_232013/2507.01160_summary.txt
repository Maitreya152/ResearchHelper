This paper introduces a novel event-based metric for evaluating abstractive news summarization, focusing on the overlap of events between generated summaries, human-authored summaries, and original news articles. The approach leverages event extraction to identify and structure event information, adapting standard event extraction evaluation metrics to quantify event overlap. Experiments on the Norwegian dataset NorSumm, which includes both event annotations (EDEN) and human-authored summaries, demonstrate the usefulness of the proposed metric in providing deeper insights into the semantic content of summaries. The study compares several open-source Norwegian and Nordic Large Language Models (LLMs), revealing differences in their ability to capture core events compared to human summarizers. The event-overlap metric offers a complementary perspective to traditional metrics like ROUGE and BERTScore, highlighting the varying focuses of LLMs and humans when summarizing news articles. The paper concludes by discussing limitations and potential future directions, emphasizing the adaptability of the metric to other datasets and languages.
