The paper introduces the MALIBU benchmark to evaluate implicit biases in multi-agent systems using Large Language Models (LLMs). It highlights the subtle nature of implicit biases compared to overt ones, and how these biases can be reinforced in multi-agent systems, leading to unfair outcomes. MALIBU assesses bias through scenario-based tasks where LLMs with assigned demographic personas (e.g., gender, race, religion) complete tasks. Their responses are evaluated by a multi-agent judging system in two phases: 1) scoring responses labeled with personas across four metrics (Creativity, Accuracy, Efficiency, Reliability), and 2) comparing paired responses assigned to different personas to identify superior responses. The research demonstrates that bias mitigation can sometimes lead to overcompensation, favoring marginalized personas, and emphasizes the necessity for careful bias detection, balanced fairness strategies, and transparent evaluation benchmarks. The experiments, conducted using GPT-4o mini and DeepSeek-V3, reveal varying degrees of bias across different demographic categories and models, pointing to the complexities of achieving true neutrality in LLM-based multi-agent systems. The study also addresses the need for future research to focus on more precise mitigation techniques to ensure balanced and fair decision-making in AI models.
